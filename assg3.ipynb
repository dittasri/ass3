{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bed7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = f\"{base_url}/s?k={product}\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract product details\n",
    "        products = soup.find_all('div', {'class': 's-result-item'})\n",
    "        for product in products:\n",
    "            product_title = product.find('span', {'class': 'a-text-normal'})\n",
    "            if product_title:\n",
    "                print(product_title.text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon: \"FLAIR Srx 0.7mm Retractable Ball Pen Box Pack | Triangular Body Design For Better Grip | Light Weight Refillable |)\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa514e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_product_details(product):\n",
    "    brand = product.find('span', {'class': 'a-size-base-plus'}).get_text(strip=True) if product.find('span', {'class': 'a-size-base-plus'}) else '-'\n",
    "    product_name = product.find('span', {'class': 'a-text-normal'}).get_text(strip=True) if product.find('span', {'class': 'a-text-normal'}) else '-'\n",
    "    price = product.find('span', {'class': 'a-offscreen'}).get_text(strip=True) if product.find('span', {'class': 'a-offscreen'}) else '-'\n",
    "    return_exchange = product.find('div', {'class': 'a-row a-size-base a-color-secondary'}).get_text(strip=True) if product.find('div', {'class': 'a-row a-size-base a-color-secondary'}) else '-'\n",
    "    expected_delivery = product.find('span', {'class': 'a-text-bold'}).get_text(strip=True) if product.find('span', {'class': 'a-text-bold'}) else '-'\n",
    "    availability = product.find('span', {'class': 'a-size-medium a-color-success'}).get_text(strip=True) if product.find('span', {'class': 'a-size-medium a-color-success'}) else '-'\n",
    "    product_url = base_url + product.find('a', {'class': 'a-link-normal'})['href'] if product.find('a', {'class': 'a-link-normal'}) else '-'\n",
    "\n",
    "    return {\n",
    "        'Brand Name': brand,\n",
    "        'Name of the Product': product_name,\n",
    "        'Price': price,\n",
    "        'Return/Exchange': return_exchange,\n",
    "        'Expected Delivery': expected_delivery,\n",
    "        'Availability': availability,\n",
    "        'Product URL': product_url\n",
    "    }\n",
    "\n",
    "def search_amazon(product):\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = f\"{base_url}/s?k={product}\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    all_products_data = []\n",
    "\n",
    "    try:\n",
    "        for page in range(1, 4):  # Scraping first 3 pages\n",
    "            page_url = f\"{search_url}&page={page}\"\n",
    "            response = requests.get(page_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract product details\n",
    "            products = soup.find_all('div', {'class': 's-result-item'})\n",
    "            for product in products:\n",
    "                product_data = extract_product_details(product)\n",
    "                all_products_data.append(product_data)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return all_products_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search on Amazon: \")\n",
    "    product_data = search_amazon(user_input)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(product_data)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv('amazon_search_results.csv', index=False)\n",
    "\n",
    "    print(\"Data saved to 'amazon_search_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10189ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def get_image_urls(search_query, num_images=10):\n",
    "    url = \"https://www.google.com/imghp\"\n",
    "\n",
    "    # Specify the path to your chromedriver executable\n",
    "    chromedriver_path = \"/path/to/chromedriver\"\n",
    "\n",
    "    # Set up a Chrome webdriver\n",
    "    driver = webdriver.Chrome(executable_path=chromedriver_path)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Locate the search bar\n",
    "    search_box = driver.find_element(\"name\", \"q\")\n",
    "\n",
    "    # Enter the search query\n",
    "    search_box.send_keys(search_query)\n",
    "\n",
    "    # Press Enter\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Scroll down to load more images (optional)\n",
    "    for _ in range(3):  # Scroll down 3 times\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Extract image URLs\n",
    "    image_urls = []\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    for img_tag in soup.find_all('img', {'class': 'rg_i'}):\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            # Some URLs are base64-encoded, skip those\n",
    "            if not img_url.startswith('data:image'):\n",
    "                image_urls.append(img_url)\n",
    "\n",
    "        # Break when we have enough images\n",
    "        if len(image_urls) >= num_images:\n",
    "            break\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "def download_images(image_urls, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for i, img_url in enumerate(image_urls):\n",
    "        try:\n",
    "            response = requests.get(img_url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Get the file extension from the URL\n",
    "            parsed_url = urlparse(img_url)\n",
    "            file_extension = os.path.splitext(parse_qs(parsed_url.query)['imgurl'][0])[-1]\n",
    "\n",
    "            # Save the image\n",
    "            with open(os.path.join(output_directory, f\"{i+1}{file_extension}\"), 'wb') as img_file:\n",
    "                img_file.write(response.content)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading image {i+1}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_queries = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    num_images_per_query = 10\n",
    "\n",
    "    for query in search_queries:\n",
    "        print(f\"Searching and downloading images for '{query}'...\")\n",
    "        image_urls = get_image_urls(query, num_images_per_query)\n",
    "        download_images(image_urls, f\"{query}_images\")\n",
    "\n",
    "    print(\"Image scraping completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_smartphone_details(product):\n",
    "    brand_name = product.find('div', {'class': '_4rR01T'}).get_text(strip=True) if product.find('div', {'class': '_4rR01T'}) else '-'\n",
    "    smartphone_name = product.find('a', {'class': 'IRpwTa'}).get_text(strip=True) if product.find('a', {'class': 'IRpwTa'}) else '-'\n",
    "    color = product.find('div', {'class': '_2WkVRV'}).get_text(strip=True) if product.find('div', {'class': '_2WkVRV'}) else '-'\n",
    "    ram = product.find('li', {'class': 'rgWa7D'}).get_text(strip=True) if product.find('li', {'class': 'rgWa7D'}) else '-'\n",
    "    storage = product.find('li', {'class': 'rgWa7D'}).find_next('li').get_text(strip=True) if product.find('li', {'class': 'rgWa7D'}).find_next('li') else '-'\n",
    "    primary_camera = product.find('li', {'class': 'rgWa7D'}).find_next('li').find_next('li').get_text(strip=True) if product.find('li', {'class': 'rgWa7D'}).find_next('li').find_next('li') else '-'\n",
    "    secondary_camera = product.find('li', {'class': 'rgWa7D'}).find_next('li').find_next('li').find_next('li').get_text(strip=True) if product.find('li', {'class': 'rgWa7D'}).find_next('li').find_next('li').find_next('li') else '-'\n",
    "    display_size = product.find('li', {'class': 'rgWa7D'}).find_next('li').find_next('li').find_next('li').find_next('li').get_text(strip=True) if product.find('li', {'class': 'rgWa7D'}).find_next('li').find_next('li').find_next('li').find_next('li') else '-'\n",
    "    battery_capacity = product.find('li', {'class': 'rgWa7D'}).find_next('li').find_next('li').find_next('li').find_next('li').find_next('li').get_text(strip=True) if product.find('li', {'class': 'rgWa7D'}).find_next('li').find_next('li').find_next('li').find_next('li').find_next('li') else '-'\n",
    "    price = product.find('div', {'class': '_30jeq3 _1_WHN1'}).get_text(strip=True) if product.find('div', {'class': '_30jeq3 _1_WHN1'}) else '-'\n",
    "    product_url = 'https://www.flipkart.com' + product.find('a', {'class': 'IRpwTa'})['href'] if product.find('a', {'class': 'IRpwTa'}) else '-'\n",
    "\n",
    "    return {\n",
    "        'Brand Name': brand_name,\n",
    "        'Smartphone Name': smartphone_name,\n",
    "        'Colour': color,\n",
    "        'RAM': ram,\n",
    "        'Storage (ROM)': storage,\n",
    "        'Primary Camera': primary_camera,\n",
    "        'Secondary Camera': secondary_camera,\n",
    "        'Display Size': display_size,\n",
    "        'Battery Capacity': battery_capacity,\n",
    "        'Price': price,\n",
    "        'Product URL': product_url\n",
    "    }\n",
    "\n",
    "def search_flipkart(product):\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = f\"{base_url}/search?q={product}\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    all_products_data = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract smartphone details\n",
    "        products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "        for product in products:\n",
    "            product_data = extract_smartphone_details(product)\n",
    "            all_products_data.append(product_data)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return all_products_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the smartphone you want to search on Flipkart: \")\n",
    "    product_data = search_flipkart(user_input)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(product_data)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv('flipkart_smartphone_search_results.csv', index=False)\n",
    "\n",
    "    print(\"Data saved to 'flipkart_smartphone_search_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_geolocation(api_key, address):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "\n",
    "    params = {\n",
    "        'address': address,\n",
    "        'key': api_key,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        if data['status'] == 'OK':\n",
    "            location = data['results'][0]['geometry']['location']\n",
    "            latitude = location['lat']\n",
    "            longitude = location['lng']\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            print(f\"Geocoding failed. Status: {data['status']}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'YOUR_API_KEY' with your actual Google Cloud Platform API key\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "    \n",
    "    # Input the city name or address you want to get geolocation for\n",
    "    city_name = input(\"Enter the city name or address: \")\n",
    "\n",
    "    coordinates = get_geolocation(api_key, city_name)\n",
    "\n",
    "    if coordinates:\n",
    "        print(f\"Geospatial Coordinates (Latitude, Longitude) for {city_name}: {coordinates}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve geospatial coordinates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    base_url = \"https://www.forbes.com/billionaires/\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        billionaires_data = []\n",
    "\n",
    "        for row in soup.select('.table-row'):\n",
    "            rank = row.select_one('.rank').get_text(strip=True)\n",
    "            name = row.select_one('.personName').get_text(strip=True)\n",
    "            net_worth = row.select_one('.netWorth').get_text(strip=True)\n",
    "            age = row.select_one('.age').get_text(strip=True)\n",
    "            citizenship = row.select_one('.countryOfCitizenship').get_text(strip=True)\n",
    "            source = row.select_one('.source-column').get_text(strip=True)\n",
    "            industry = row.select_one('.category').get_text(strip=True)\n",
    "\n",
    "            billionaire_info = {\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net Worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            }\n",
    "\n",
    "            billionaires_data.append(billionaire_info)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    return billionaires_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires_data = scrape_forbes_billionaires()\n",
    "\n",
    "    if billionaires_data:\n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(billionaires_data)\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        df.to_csv('forbes_billionaires.csv', index=False)\n",
    "\n",
    "        print(\"Data saved to 'forbes_billionaires.csv'\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "from datetime import datetime\n",
    "\n",
    "def get_authenticated_service(api_key):\n",
    "    return googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "def get_video_comments(service, **kwargs):\n",
    "    comments = []\n",
    "    results = service.commentThreads().list(**kwargs).execute()\n",
    "\n",
    "    while results:\n",
    "        for item in results['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'text': comment['textDisplay'],\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'upvotes': comment['likeCount'],\n",
    "                'published_at': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        # Check if there are more comments\n",
    "        if 'nextPageToken' in results:\n",
    "            kwargs['pageToken'] = results['nextPageToken']\n",
    "            results = service.commentThreads().list(**kwargs).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "def main(api_key, video_id):\n",
    "    service = get_authenticated_service(api_key)\n",
    "\n",
    "    comments = get_video_comments(\n",
    "        service,\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        textFormat='plainText',\n",
    "        order='time',  # You can change the order if needed\n",
    "        maxResults=500\n",
    "    )\n",
    "\n",
    "    for i, comment in enumerate(comments, start=1):\n",
    "        print(f\"Comment {i}:\")\n",
    "        print(f\"Text: {comment['text']}\")\n",
    "        print(f\"Author: {comment['author']}\")\n",
    "        print(f\"Upvotes: {comment['upvotes']}\")\n",
    "        published_at = datetime.strptime(comment['published_at'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        print(f\"Published At: {published_at}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'YOUR_API_KEY' with your actual Google Cloud Platform API key\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    # Replace 'VIDEO_ID' with the actual video ID you want to fetch comments for\n",
    "    video_id = 'VIDEO_ID'\n",
    "\n",
    "    main(api_key, video_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f132d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    base_url = \"https://www.hostelworld.com\"\n",
    "    search_url = f\"{base_url}/search?search_keywords=London&country=England&city=London&type=city&id=3&from=2023-12-01&to=2023-12-02&guests=1&page=1\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        hostels_data = []\n",
    "\n",
    "        for hostel_card in soup.select('.property-card'):\n",
    "            hostel_name = hostel_card.select_one('.property-name').get_text(strip=True)\n",
    "            distance_from_city_center = hostel_card.select_one('.property-card__distance').get_text(strip=True)\n",
    "            ratings = hostel_card.select_one('.score-orange').get_text(strip=True)\n",
    "            total_reviews = hostel_card.select_one('.reviews').get_text(strip=True)\n",
    "            overall_reviews = hostel_card.select_one('.keyword').get_text(strip=True)\n",
    "            privates_from_price = hostel_card.select_one('.price-col-private .price').get_text(strip=True)\n",
    "            dorms_from_price = hostel_card.select_one('.price-col-dorm .price').get_text(strip=True)\n",
    "            facilities = ', '.join([facility.get_text(strip=True) for facility in hostel_card.select('.facilities-list-item')])\n",
    "            property_description = hostel_card.select_one('.property-card__description').get_text(strip=True)\n",
    "\n",
    "            hostel_info = {\n",
    "                'Hostel Name': hostel_name,\n",
    "                'Distance from City Centre': distance_from_city_center,\n",
    "                'Ratings': ratings,\n",
    "                'Total Reviews': total_reviews,\n",
    "                'Overall Reviews': overall_reviews,\n",
    "                'Privates from Price': privates_from_price,\n",
    "                'Dorms from Price': dorms_from_price,\n",
    "                'Facilities': facilities,\n",
    "                'Property Description': property_description\n",
    "            }\n",
    "\n",
    "            hostels_data.append(hostel_info)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    return hostels_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hostels_data = scrape_hostels_in_london()\n",
    "\n",
    "    if hostels_data:\n",
    "        for i, hostel_info in enumerate(hostels_data, start=1):\n",
    "            print(f\"Hostel {i}:\")\n",
    "            for key, value in hostel_info.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da609ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37b9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624242f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc42bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
